# Vehicle Damage Assessment Agentic Workflow

## Description

This Python script automates the process of assessing vehicle damage from images using an agentic workflow involving multiple calls to the Azure OpenAI API (specifically targeting a GPT-4o deployment).

The script reads a list of image files from a CSV, processes each image using a two-step AI approach (Assessment and Quality Control), utilizes threading for parallel processing, and outputs the final damage predictions to a new CSV file.

## Workflow

1.  **Read Input Data:** The script starts by reading `task3_dataset/test.csv`. It expects this file to have columns named `id` and `file_name`. It extracts the image ID and constructs the full path to the image file (assuming images are in `task3_dataset/test/`).
2.  **Agentic Processing (for each image):**
    * **Assessor Agent:** The `assessor_agent` function is called three times for the same image, each with a different `temperature` setting (0.0, 0.5, 1.0). This agent uses GPT-4o to analyze the image and generate a description of visible damage based on predefined categories (Dent, Scratch, Crack, Shattered glass, Broken lamp, Flat tire).
    * **QAQC Agent:** The `qaqc_agent` function is then called. It receives the image and the *last* prediction generated by the `assessor_agent` (*Note: See Bug section below*). This agent's role is intended to review the provided prediction(s) against the image, select the best one, potentially merge details, and output a final, refined prediction in JSON format.
3.  **Parallel Processing:** The script divides the list of images into chunks and processes these chunks concurrently using multiple threads via the `threading` module to speed up execution.
4.  **Write Output Data:** After all images are processed, the script compiles the results (mapping image `id` to the final `prediction` from the QAQC agent) and writes them to `task3_dataset/predictions.csv`.

## Features

* **Multi-Agent AI Workflow:** Uses separate "Assessor" and "QAQC" agent prompts for damage analysis and refinement.
* **Leverages GPT-4o:** Utilizes Azure OpenAI's GPT-4o model for vision and language understanding capabilities.
* **Variable Temperature Sampling:** Explores different prediction possibilities by calling the Assessor agent with multiple temperature settings.
* **Parallel Processing:** Employs threading to process multiple images concurrently, improving performance.
* **CSV Input/Output:** Reads image lists from a CSV and writes predictions to a CSV.

## Prerequisites

* Python 3.x
* Required Python libraries: `openai`, `python-dotenv` (recommended for API keys). Install using:
    ```bash
    pip install openai python-dotenv Pillow # Pillow is imported but not used
    ```
* Azure OpenAI Account: Access to an Azure OpenAI endpoint and an API key with permissions for a GPT-4o model deployment.
* **Input Data Structure:**
    * A CSV file named `test.csv` located in a `task3_dataset` subdirectory, containing `id` and `file_name` columns.
    * Corresponding image files located in a `task3_dataset/test/` subdirectory.

## Setup

1.  **Clone/Download:** Place the script in your desired project directory.
2.  **Install Dependencies:** Run `pip install openai python-dotenv Pillow`.
3.  **Configure API Credentials (IMPORTANT):**
    * **DO NOT USE HARDCODED KEYS IN PRODUCTION.** The script currently has hardcoded endpoint URLs and API keys in the `qaqc_agent` and `assessor_agent` functions. This is a significant security risk.
    * **Recommended:** Modify the script to use environment variables.
        * Create a file named `.env` in the same directory as the script.
        * Add your Azure OpenAI details to the `.env` file:
            ```dotenv
            AZURE_OPENAI_ENDPOINT="YOUR_AZURE_ENDPOINT_HERE"
            AZURE_OPENAI_KEY="YOUR_AZURE_API_KEY_HERE"
            AZURE_OPENAI_DEPLOYMENT="YOUR_GPT4O_DEPLOYMENT_NAME"
            AZURE_OPENAI_API_VERSION="2024-08-01-preview" # Or your API version
            ```
        * Modify the Python script:
            * Add `import os` and `from dotenv import load_dotenv` at the top.
            * Add `load_dotenv()` near the beginning of the script (e.g., in the `if __name__ == "__main__":` block).
            * Replace the hardcoded strings in `qaqc_agent` and `assessor_agent` with `os.getenv("VARIABLE_NAME")`, like:
                ```python
                endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
                subscription_key = os.getenv("AZURE_OPENAI_KEY")
                deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")
                api_version = os.getenv("AZURE_OPENAI_API_VERSION")
                ```
4.  **Prepare Input Data:** Ensure `task3_dataset/test.csv` and the corresponding images in `task3_dataset/test/` are present and correctly formatted.

## Usage

1.  Navigate to the script's directory in your terminal.
2.  Run the script:
    ```bash
    python your_script_name.py
    ```
    (Replace `your_script_name.py` with the actual filename).
3.  The script will print progress updates indicating the number of files processed.
4.  Upon completion, the results will be saved in `task3_dataset/predictions.csv`.

## Code Structure

* `read_data(input_file)`: Reads the input CSV and prepares the ID-to-filename mapping.
* `write_predictions_to_csv(id_to_prediction, output_file)`: Writes the final predictions to the output CSV.
* `encode_image_to_base64(self, image_path)`: Defined but **unused** in the main workflow (encoding is done directly within agents).
* `qaqc_agent(image_path, predictions)`: Implements the Quality Control AI agent call using Azure OpenAI.
* `assessor_agent(image_path, temperatures)`: Implements the Damage Assessor AI agent call using Azure OpenAI.
* `agentic_flow(image_path)`: Orchestrates the calls to the Assessor and QAQC agents for a single image.
* `process_chunk(chunk, id_to_prediction, lock)`: Function executed by each thread to process a batch of images.
* `if __name__ == "__main__":`: Main execution block that sets up threading, runs the processing, and saves the results.

## Important Notes & Potential Issues

* **Security Warning:** The script contains hardcoded API credentials. **Modify it to use environment variables or another secure method before use.**
* **Bug in `agentic_flow`:** The `qaqc_agent` is currently called with `prediction` (the *last* prediction from the Assessor loop) instead of the list `predictions`. This means the QAQC agent only reviews one prediction, not the intended three, limiting its effectiveness in comparing and merging results.
* **Redundancy:** Azure OpenAI client configuration and image base64 encoding logic are duplicated in both `qaqc_agent` and `assessor_agent`. This could be refactored into helper functions or a shared client instance.
* **Hardcoded Chunking:** The threading logic uses hardcoded chunk sizes (34) and chunk counts (11). This might not be optimal or robust for varying numbers of input files.
* **Unused Code:** The `encode_image_to_base64` function and the `PIL` import are not actively used in the final workflow.
* **Leftover Code:** The final `agentic_flow("task3_dataset/test/002235.jpg")` call at the end of the script seems like leftover debugging code, as its result is not used.

## Potential Improvements

* Implement secure handling of API keys using environment variables (`python-dotenv`) or a configuration file.
* Fix the bug in `agentic_flow` to correctly pass the list of all assessor predictions to the `qaqc_agent`.
* Refactor API client setup and image encoding to avoid duplication.
* Remove unused functions and imports.
* Make the threading chunk size dynamic or configurable.
* Add more robust error handling for API calls, file operations, and JSON parsing.
* Consider adding command-line arguments for input/output file paths, API details, and thread count.